# ğŸ™ï¸ FLUJO COMPLETO DE AUDIO - Widget Sandra

## ğŸ“‹ Problema Identificado

El servidor MCP solo transcribÃ­a el audio pero **NO generaba ni enviaba la respuesta de audio de Sandra**.

## âœ… SoluciÃ³n Implementada

### Flujo Completo Corregido:

1. **Cliente graba audio del usuario**
   - MediaRecorder captura audio del micrÃ³fono
   - Se envÃ­a como base64 en formato WebM

2. **Cliente envÃ­a al servidor MCP**
   ```json
   {
     "route": "audio",
     "action": "stt",
     "payload": {
       "audio": "base64Audio...",
       "format": "webm",
       "mimeType": "audio/webm;codecs=opus"
     }
   }
   ```

3. **Servidor MCP procesa** (CORREGIDO):
   - âœ… Transcribe audio â†’ Obtiene texto del usuario
   - âœ… Procesa con IA (Qwen/Conserje) â†’ Obtiene respuesta de texto
   - âœ… Genera audio con TTS (Cartesia) â†’ Convierte respuesta a audio
   - âœ… EnvÃ­a audio de vuelta al cliente

4. **Servidor MCP envÃ­a respuesta**:
   ```json
   {
     "route": "audio",
     "action": "tts",
     "payload": {
       "audio": "base64AudioResponse...",
       "format": "mp3",
       "text": "Respuesta de Sandra",
       "isWelcome": false
     }
   }
   ```

5. **Cliente recibe y reproduce**
   - Detecta `route: 'audio', action: 'tts'`
   - Extrae `payload.audio`
   - Llama a `playAudioResponse(payload.audio, payload.isWelcome)`
   - Reproduce audio de Sandra

## ğŸ”§ Cambios Realizados

### `mcp-server/index.js` - FunciÃ³n `handleAudioRoute`

**ANTES:**
```javascript
case 'stt':
  const transcript = await services.transcriber.transcribe(payload.audio);
  return { transcript }; // âŒ Solo devolvÃ­a texto, NO audio
```

**DESPUÃ‰S:**
```javascript
case 'stt':
  // 1. Transcribir
  const transcript = await services.transcriber.transcribe(payload.audio);
  
  // 2. Procesar con IA
  const aiResponse = await services.qwen.processMessage(transcript, {...});
  
  // 3. Generar audio (TTS)
  const responseAudio = await services.cartesia.textToSpeech(aiResponse);
  
  // 4. Enviar audio directamente por WebSocket
  ws.send(JSON.stringify({
    route: 'audio',
    action: 'tts',
    payload: {
      audio: responseAudio,
      format: 'mp3',
      text: aiResponse,
      isWelcome: payload.isWelcome || false
    }
  }));
  
  return { transcript, text: aiResponse, audioSent: true };
```

## ğŸ¯ Flujo Completo de la ConversaciÃ³n

```
Usuario habla
    â†“
[MediaRecorder] Graba audio (WebM)
    â†“
[Cliente] EnvÃ­a {route: 'audio', action: 'stt', payload: {audio: base64}}
    â†“
[MCP Server] handleAudioRoute('stt', payload)
    â†“
    â”œâ”€â†’ [Transcriber] Transcribe audio â†’ "Hola Sandra"
    â†“
    â”œâ”€â†’ [Qwen/IA] Procesa texto â†’ "Â¡Hola! Â¿En quÃ© puedo ayudarte?"
    â†“
    â”œâ”€â†’ [Cartesia/TTS] Genera audio â†’ base64Audio
    â†“
    â””â”€â†’ [WebSocket] EnvÃ­a {route: 'audio', action: 'tts', payload: {audio: base64Audio}}
         â†“
[Cliente] ws.onmessage recibe mensaje
    â†“
[Cliente] Detecta route='audio' && action='tts'
    â†“
[Cliente] Extrae payload.audio
    â†“
[Cliente] playAudioResponse(payload.audio)
    â†“
[Cliente] Crea Audio element y reproduce
    â†“
Usuario escucha respuesta de Sandra
```

## âš ï¸ Notas Importantes

1. **Saludo inicial**: Cuando es el primer mensaje, el servidor debe enviar `isWelcome: true` para que el cliente sincronice el video del hero correctamente.

2. **No speech**: Si no hay transcripciÃ³n vÃ¡lida, el servidor envÃ­a un mensaje de tipo `noSpeech` en lugar de audio.

3. **Barge-in**: El cliente detiene la grabaciÃ³n cuando Sandra empieza a hablar para evitar feedback loops.

4. **Formato**: 
   - Audio enviado por cliente: WebM (opus codec)
   - Audio recibido por cliente: MP3 (generado por Cartesia)

## ğŸš€ PrÃ³ximos Pasos

1. Probar el flujo completo en producciÃ³n
2. Verificar que el audio se reproduce correctamente
3. Asegurar que el saludo inicial funciona con sincronizaciÃ³n de video
4. Probar casos edge: silencio, errores de transcripciÃ³n, errores de TTS

